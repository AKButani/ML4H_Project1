{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60836629-ca4b-4878-9b3e-08f7c01b78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1926\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "seed = 42 # Set the seed for reproducibility\n",
    "random.seed(seed) # For Python's built-in random module (if you use it anywhere)\n",
    "\n",
    "# Path to the directory where the prompt files are stored\n",
    "prompt_directory = \"Q4P1txt\"\n",
    "output_directory = \"Q4p1_results_mistral\"\n",
    "\n",
    "# List all files in the directory\n",
    "prompt_files = [f for f in os.listdir(prompt_directory) if f.endswith('.txt')]\n",
    "\n",
    "\n",
    "# Get length of prompt_files\n",
    "print(f\"Total files: {len(prompt_files)}\")\n",
    "\n",
    "# Define the role text\n",
    "role_txt = \"\"\"You are a helpful assistant to a doctor and want to predict whether the case patients die or survive. Return for the last case whether you think the patient will die or survive based on their values, considering the examples given, and that the death rate for 4000 cases is only 14%. State your response only and exactly with the string 'outcome:survival Index:' or 'outcome:death Index:' plus the original index number.\"\"\"\n",
    "\n",
    "# Function to call the local Mistral model with retry mechanism\n",
    "def call_mistral_with_retry(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            # Combine the role text with the prompt\n",
    "            full_prompt = f\"{role_txt}\\n{prompt}\"\n",
    "            \n",
    "            # Run the Mistral model via the ollama CLI\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", \"gemma2:2b\", full_prompt],\n",
    "                capture_output=True, text=True, encoding='utf-8'  # Explicitly set encoding to UTF-8\n",
    "            )\n",
    "            \n",
    "            # Check for errors\n",
    "            if result.returncode != 0:\n",
    "                raise Exception(f\"Error calling Mistral: {result.stderr}\")\n",
    "\n",
    "            # Get the generated response\n",
    "            response = result.stdout.strip()\n",
    "            return response\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle exceptions (e.g., errors in subprocess call)\n",
    "            print(f\"Error occurred: {e}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2939adfa-11b5-47bb-9862-ac40a56d4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Q4p1_prompts_list.pkl', 'rb') as file:\n",
    "    prompts_list_from_file = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189f9943-fd84-443f-a3f0-cb994bf8ac44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_list_from_file)\n",
    "#print(prompts_list_from_file[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2d52df-d47b-457c-b9e3-bbcdbb9427ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from ollama import chat\n",
    "\n",
    "# Iterate over the prompts_list_from_file instead of files\n",
    "responses = []  # List to store the responses for all prompts\n",
    "\n",
    "# Define the role text\n",
    "role_txt = \"\"\"You are a helpful assistant to a doctor and want to predict whether the case patients die or survive. Return for the last case whether you think the patient will die or survive based on their values, considering the examples given, and that the death rate for 4000 cases is only 14%. State your response only and exactly with the string 'outcome:survival Index:' or 'outcome:death Index:' plus the original index number, and \"Score:\", + a score of 1 to 100 which is the probability that the patient is to die (100= certain death).\"\"\"\n",
    "# Save the responses in a pickle file\n",
    "output_pickle_path = 'Q4p1_responses_all.pkl'\n",
    "\n",
    "run_script = False\n",
    "\n",
    "if run_script:\n",
    "    for prompt in prompts_list_from_file[:]:\n",
    "        # Append the role_txt to the prompt\n",
    "        prompt = prompt + \"\\n\" + role_txt\n",
    "    \n",
    "        # Call Ollama model with streaming responses for each prompt\n",
    "        stream = chat(\n",
    "            model='gemma2:2b',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=True,\n",
    "        )\n",
    "    \n",
    "        # Collect the response and process it\n",
    "        response = ''\n",
    "        for chunk in stream:\n",
    "            response += chunk['message']['content']\n",
    "            # Uncomment the next line if you want to print as the response streams\n",
    "            # print(chunk['message']['content'], end='', flush=True)\n",
    "    \n",
    "        # Add the response to the list\n",
    "        responses.append(response)\n",
    "    \n",
    "        print(\"\\n\")  # Add a newline after processing each prompt\n",
    "        \n",
    "        with open(output_pickle_path, 'wb') as file:\n",
    "            pickle.dump(responses, file)\n",
    "    \n",
    "    print(f\"Responses saved as '{output_pickle_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e35cf6-a97d-4af2-b718-3854ea333226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the responses in a pickle file\n",
    "output_pickle_path = 'Q4p1_responses_all.pkl'\n",
    "with open(output_pickle_path, 'rb') as file:\n",
    "    pkl_data_all = pickle.load(file)\n",
    "len(pkl_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab57e468-965f-48f6-adb6-103b3e71586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to hold all data\n",
    "all_data = []\n",
    "\n",
    "# Function to clean the file name (if needed)\n",
    "def clean_file_name(file_name):\n",
    "    return file_name.replace(\"outcome:death\", \"\").replace(\"outcome:survival\", \"\")\n",
    "\n",
    "# Function to clean the file name (extract the index number from the entry)\n",
    "def extract_index(entry):\n",
    "    match = re.search(r\"Index:\\s*(\\d+)\", entry)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the index number as string\n",
    "    return \"unknown\"  # If no index is found, return \"unknown\"\n",
    "\n",
    "# Function to extract the score from the entry\n",
    "def extract_score(entry):\n",
    "    match = re.search(r\"Score:\\s*(\\d+(\\.\\d+)?)\", entry)  # Matches integers or decimals\n",
    "    if match:\n",
    "        return match.group(1)  # Return the score as string\n",
    "    return \"unknown\"  # If no score is found, return \"unknown\"\n",
    "\n",
    "# Parse the pkl data and extract information\n",
    "for entry in pkl_data_all:\n",
    "    if \"outcome:death\" in entry or \"outcome:survival\" in entry:\n",
    "        # Extract the outcome\n",
    "        outcome = entry.split(' ')[0]  # Assuming the outcome is the first word in the entry (outcome:death or outcome:survival)\n",
    "        \n",
    "        # Extract the index number to use as the file name\n",
    "        file_name = extract_index(entry)\n",
    "        \n",
    "        # Extract the score\n",
    "        score = extract_score(entry)\n",
    "        \n",
    "        # Add to the all_data list\n",
    "        all_data.append({\"RecordID\": file_name, \"outcome\": outcome, \"score\": score})\n",
    "\n",
    "# If you want to print or further inspect the data\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283a64f4-5299-4fd1-80cd-78992ec508ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RecordID           outcome score\n",
      "0   152871  outcome:survival    90\n",
      "1   152873  outcome:survival    90\n",
      "2   152875  outcome:survival    90\n",
      "3   152878  outcome:survival    90\n",
      "4   152882  outcome:survival    75\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "3993    1\n",
      "3994    0\n",
      "3995    0\n",
      "3996    1\n",
      "3997    1\n",
      "Name: pred, Length: 3998, dtype: int64\n",
      "0       152871\n",
      "1       152873\n",
      "2       152875\n",
      "3       152878\n",
      "4       152882\n",
      "         ...  \n",
      "3993    163029\n",
      "3994    163033\n",
      "3995    163034\n",
      "3996    163035\n",
      "3997    163037\n",
      "Name: RecordID, Length: 3998, dtype: object\n",
      "object\n",
      "Int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(all_data)\n",
    "print(df.head())\n",
    "df['pred'] = df['outcome'].apply(lambda x: 1 if \"outcome:death\" in x else 0)\n",
    "\n",
    "print(df[\"pred\"])\n",
    "print(df['RecordID'])\n",
    "\n",
    "df.head()\n",
    "# Check the type of the 'outcome2' column\n",
    "print(df['RecordID'].dtype)\n",
    "df['RecordID'] = pd.to_numeric(df['RecordID'], errors='coerce').astype('Int64')  # Using 'Int64' to allow NaN support\n",
    "print(df['RecordID'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a602b6f-346a-466a-b777-5e0804041228",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df_path = 'loaded_data/c_patient_data_NOT_scaled.parquet'\n",
    "# read the parquet file\n",
    "testing_df = pd.read_parquet(testing_df_path)\n",
    "\n",
    "testing_df[\"In-hospital_death\"].head()\n",
    "testing_df[\"RecordID\"].head()\n",
    "# Ensure the 'RecordID' column is of integer type\n",
    "testing_df[\"RecordID\"] = testing_df[\"RecordID\"].astype(int)\n",
    "testing_df.head()\n",
    "\n",
    "\n",
    "# remove blank spaces in the RecordID column\n",
    "df[\"RecordID\"]\n",
    "df.head()\n",
    "df.head()\n",
    "df_true_vals = testing_df[[\"In-hospital_death\", \"RecordID\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc707f-4c09-4839-856c-81dceaa1244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "# Merge the two dataframes on the 'RecordID' column\n",
    "merged_df = df_true_vals.merge(df, left_on=\"RecordID\", right_on=\"RecordID\", how=\"inner\")\n",
    "print(merged_df.head(200))\n",
    "\n",
    "# Perform sensitivity analysis\n",
    "y_true = merged_df[\"In-hospital_death\"]\n",
    "y_pred = merged_df[\"pred\"]\n",
    "y_pred\n",
    "\n",
    "# Calculate sensitivity (recall for the positive class)\n",
    "sensitivity = recall_score(y_true, y_pred)\n",
    "\n",
    "# Print the sensitivity and other metrics\n",
    "print(\"Sensitivity (Recall):\", sensitivity)\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "\n",
    "# Optional: Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76bf70-0ae8-4b72-91d1-5fc5de57a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_df['score'] = pd.to_numeric(merged_df['score'], errors='coerce') \n",
    "\n",
    "# Plotting Score vs In-hospital_death\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(merged_df['score'], merged_df['In-hospital_death'], c=merged_df['In-hospital_death'], cmap='bwr', alpha=0.7)\n",
    "plt.title('Score vs In-hospital Death')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('In-hospital Death')\n",
    "plt.grid(True)\n",
    "plt.colorbar(label='In-hospital Death (0=Survival, 1=Death)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec44cdf-57e1-499b-81dc-6801321feb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "score = merged_df['score']/100\n",
    "nan_pred_vals = score.isna().sum()\n",
    "print(nan_pred_vals)\n",
    "score = score.fillna(score.mean())\n",
    "\n",
    "# Assuming y_pred_proba contains the predicted probabilities for the positive class (class 1)\n",
    "fpr, tpr, _ = roc_curve(y_true, score)\n",
    "precision, recall, _ = precision_recall_curve(y_true, score)\n",
    "\n",
    "# Calculate AUROC (Area Under the Receiver Operating Characteristic Curve)\n",
    "auroc = roc_auc_score(y_true, score)\n",
    "\n",
    "# Calculate AUPRC (Area Under the Precision-Recall Curve)\n",
    "auprc = average_precision_score(y_true, score)\n",
    "\n",
    "print(auroc)\n",
    "print(auprc)\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, color='green', lw=2)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
