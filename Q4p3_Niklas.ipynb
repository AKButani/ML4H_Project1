{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1a70f4-43c0-4b28-a547-12d4b6defc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e015089-048b-4eb7-aefe-6d332dd699f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42 # Set the seed for reproducibility\n",
    "torch.manual_seed(seed) # For PyTorch\n",
    "torch.cuda.manual_seed_all(seed)  # If you're using GPU\n",
    "np.random.seed(seed) # For NumPy (if you use NumPy anywhere)\n",
    "random.seed(seed) # For Python's built-in random module (if you use it anywhere)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e826e145-d01d-4937-887a-971baaeb7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_datasets():\n",
    "    train_set = pd.read_parquet('loaded_data/a_patient_data_processed_cluster.parquet')\n",
    "    test_set = pd.read_parquet('loaded_data/c_patient_data_processed_cluster.parquet')\n",
    "    return train_set, test_set\n",
    "\n",
    "train_df, test_df = load_datasets()\n",
    "\n",
    "X_train = torch.tensor(train_df.drop(columns=[\"ICUType\", \"In-hospital_death\"]).values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_df[\"In-hospital_death\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test = torch.tensor(test_df.drop(columns=[\"ICUType\", \"In-hospital_death\"]).values, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_df[\"In-hospital_death\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "feature_columns = [col for col in train_df.columns if col not in [\"RecordID\", \"In-hospital_death\", \"ICUType\"]]\n",
    "\n",
    "def pad_to_fixed_length(tensor, length=49):\n",
    "    current_length = tensor.size(0)\n",
    "    if current_length < length:\n",
    "        padding = torch.zeros((length - current_length, tensor.size(1)))\n",
    "        return torch.cat([tensor, padding], dim=0)\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def process_dataframe(df):\n",
    "    list_of_patient_data = []\n",
    "    patient_labels = []\n",
    "    grouped = df.groupby(\"RecordID\")\n",
    "    \n",
    "    for record_id, group in grouped:\n",
    "        # Extract features and labels\n",
    "        group_data = group[feature_columns].values\n",
    "        group_tensor = torch.tensor(group_data, dtype=torch.float32)\n",
    "        group_tensor_fixed = pad_to_fixed_length(group_tensor, length=49)\n",
    "        \n",
    "        # Keep the RecordID intact\n",
    "        patient_data = pd.DataFrame(group_tensor_fixed.numpy(), columns=feature_columns)\n",
    "        patient_data['RecordID'] = record_id  # Add the RecordID as a column\n",
    "        \n",
    "        list_of_patient_data.append(patient_data)\n",
    "        # For the label, we assume that if any timestep indicates death, the patient is labeled as death (1)\n",
    "        patient_labels.append(group[\"In-hospital_death\"].max())\n",
    "    \n",
    "    # Combine all the patient data\n",
    "    final_df = pd.concat(list_of_patient_data, ignore_index=True)\n",
    "    final_labels = pd.DataFrame(patient_labels, columns=[\"In-hospital_death\"])\n",
    "    \n",
    "    return final_df, final_labels\n",
    "\n",
    "# Process training and testing data and keep the RecordID intact\n",
    "processed_train_data, labels_tensor_train = process_dataframe(train_df)\n",
    "processed_test_data, labels_tensor_test = process_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cd0599-8a26-48f3-917f-fe71af8278ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195853, 42) (3997, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecordID\n",
       "132539.0    49\n",
       "132540.0    49\n",
       "132541.0    49\n",
       "132543.0    49\n",
       "132545.0    49\n",
       "            ..\n",
       "142665.0    49\n",
       "142667.0    49\n",
       "142670.0    49\n",
       "142671.0    49\n",
       "142673.0    49\n",
       "Length: 3997, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_train_data.shape, labels_tensor_train.shape)\n",
    "processed_train_data\n",
    "processed_train_data.groupby('RecordID').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dec1ce6-3098-4208-b176-16c4cf4c3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# Load the Chronos pipeline for time-series forecasting or embeddings\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # Use the Chronos model\n",
    "    torch_dtype=torch.float16,  # Using float16 precision for faster processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78d5068-54b6-4925-a3e6-e01219bc74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get embeddings for each variable using the Chronos pipeline\n",
    "def get_embedding_for_variable(variable_data):\n",
    "    # Extract the relevant column for time-series (e.g., 'variable_1') and convert to tensor\n",
    "    context = torch.tensor(variable_data.values, dtype=torch.float32)\n",
    "    context = context.unsqueeze(0)  # Add a batch dimension (making it a 2D tensor)\n",
    "    \n",
    "    # Generate embeddings for the time-series data\n",
    "    embeddings, tokenizer_state = pipeline.embed(context)  # Extract embeddings for the variable\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Function to compute the aggregated embedding for a patient\n",
    "def get_patient_embedding(patient_data, patient_id):\n",
    "    # Filter data for the specific patient\n",
    "    patient_df = patient_data[patient_data['RecordID'] == patient_id]\n",
    "    \n",
    "    # Create a list to store the embeddings for each variable\n",
    "    embeddings = []\n",
    "    \n",
    "    # Iterate over each variable (assuming columns are 'variable_1', 'variable_2', etc.)\n",
    "    for column in patient_df.columns:\n",
    "        if column != \"RecordID\" and column != \"Time\":\n",
    "            # Get the embedding for the current variable\n",
    "            variable_data = patient_df[column]\n",
    "            embedding = get_embedding_for_variable(variable_data)\n",
    "            \n",
    "            # Append the embedding to the list\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "    \n",
    "    # Convert the list of embeddings into a tensor and compute the average across all variables\n",
    "    embeddings_tensor = torch.stack(embeddings)\n",
    "    aggregated_embedding = torch.mean(embeddings_tensor, dim=0)  # Average embeddings across all variables\n",
    "    \n",
    "    return aggregated_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43e1527-5350-4634-81dd-fcfe90d599eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ALP       ALT       AST       Age   Albumin       BUN  Bilirubin  \\\n",
      "0      -0.138920 -0.123519 -0.118468 -0.596605  0.009690 -0.292460  -0.160848   \n",
      "1      -0.138920 -0.123519 -0.118468 -0.596605  0.009690 -0.292460  -0.160848   \n",
      "2      -0.138920 -0.123519 -0.118468 -0.596605  0.009690 -0.292460  -0.160848   \n",
      "3      -0.138920 -0.123519 -0.118468 -0.596605  0.009690 -0.292460  -0.160848   \n",
      "4      -0.138920 -0.123519 -0.118468 -0.596605  0.009690 -0.292460  -0.160848   \n",
      "...          ...       ...       ...       ...       ...       ...        ...   \n",
      "195848 -0.717583 -0.172539 -0.035137  0.781907 -2.180711 -0.044264  -0.230621   \n",
      "195849 -0.717583 -0.172539 -0.035137  0.781907 -2.180711 -0.044264  -0.230621   \n",
      "195850 -0.717583 -0.172539 -0.035137  0.781907 -2.180711 -0.044264  -0.230621   \n",
      "195851  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "195852  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "\n",
      "        Cholesterol  Creatinine   DiasABP  ...    SysABP      Temp  Time  \\\n",
      "0          -0.01868   -0.308918 -0.065600  ... -0.030791 -2.133063   1.0   \n",
      "1          -0.01868   -0.308918 -0.065600  ... -0.030791 -2.133063   2.0   \n",
      "2          -0.01868   -0.308918 -0.065600  ... -0.030791 -2.133063   3.0   \n",
      "3          -0.01868   -0.308918 -0.065600  ... -0.030791  0.972883   4.0   \n",
      "4          -0.01868   -0.308918 -0.065600  ... -0.030791  0.972883   5.0   \n",
      "...             ...         ...       ...  ...       ...       ...   ...   \n",
      "195848     -0.01868    0.128424 -0.739803  ... -0.261484  0.719336  46.0   \n",
      "195849     -0.01868    0.128424 -0.402702  ...  0.153763  0.339016  47.0   \n",
      "195850     -0.01868    0.128424 -0.065600  ...  0.476734  0.339016  48.0   \n",
      "195851      0.00000    0.000000  0.000000  ...  0.000000  0.000000   0.0   \n",
      "195852      0.00000    0.000000  0.000000  ...  0.000000  0.000000   0.0   \n",
      "\n",
      "        TroponinI  TroponinT     Urine       WBC    Weight        pH  RecordID  \n",
      "0        -0.08847  -0.146278  1.928575 -0.155492 -0.120163 -0.013711  132539.0  \n",
      "1        -0.08847  -0.146278 -0.542212 -0.155492 -0.120163 -0.013711  132539.0  \n",
      "2        -0.08847  -0.146278  0.226477 -0.155492 -0.120163 -0.013711  132539.0  \n",
      "3        -0.08847  -0.146278 -0.377493 -0.155492 -0.120163 -0.013711  132539.0  \n",
      "4        -0.08847  -0.146278 -0.377493 -0.155492 -0.120163 -0.013711  132539.0  \n",
      "...           ...        ...       ...       ...       ...       ...       ...  \n",
      "195848   -0.08847  -0.146278 -0.580647 -0.232277  0.171668 -0.027032  142673.0  \n",
      "195849   -0.08847  -0.146278 -0.487306 -0.232277  0.171668 -0.027032  142673.0  \n",
      "195850   -0.08847  -0.146278 -0.542212 -0.232277  0.171668 -0.027032  142673.0  \n",
      "195851    0.00000   0.000000  0.000000  0.000000  0.000000  0.000000  142673.0  \n",
      "195852    0.00000   0.000000  0.000000  0.000000  0.000000  0.000000  142673.0  \n",
      "\n",
      "[195853 rows x 42 columns]\n",
      "Index(['ALP', 'ALT', 'AST', 'Age', 'Albumin', 'BUN', 'Bilirubin',\n",
      "       'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Gender',\n",
      "       'Glucose', 'HCO3', 'HCT', 'HR', 'Height', 'K', 'Lactate', 'MAP',\n",
      "       'MechVent', 'Mg', 'NIDiasABP', 'NIMAP', 'NISysABP', 'Na', 'PaCO2',\n",
      "       'PaO2', 'Platelets', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'Time',\n",
      "       'TroponinI', 'TroponinT', 'Urine', 'WBC', 'Weight', 'pH', 'RecordID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load patient data from a Parquet file\n",
    "patient_data = processed_train_data\n",
    "first_5_record_ids = patient_data['RecordID'].unique()[:5]\n",
    "#patient_data = patient_data[patient_data['RecordID'].isin(first_5_record_ids)]\n",
    "print(patient_data)\n",
    "print(patient_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e2dd3a-167b-4004-8d9b-c83a2293bb89",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m all_patient_embeddings = {}\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m patient_id \u001b[38;5;129;01min\u001b[39;00m patient_data[\u001b[33m'\u001b[39m\u001b[33mRecordID\u001b[39m\u001b[33m'\u001b[39m].unique()[:]:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Get the aggregated embedding for the current patient\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     patient_embedding = \u001b[43mget_patient_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Store the aggregated embedding for each patient in a dictionary\u001b[39;00m\n\u001b[32m      9\u001b[39m     all_patient_embeddings[patient_id] = patient_embedding\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mget_patient_embedding\u001b[39m\u001b[34m(patient_data, patient_id)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m column != \u001b[33m\"\u001b[39m\u001b[33mRecordID\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m column != \u001b[33m\"\u001b[39m\u001b[33mTime\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Get the embedding for the current variable\u001b[39;00m\n\u001b[32m     24\u001b[39m     variable_data = patient_df[column]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     embedding = \u001b[43mget_embedding_for_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Append the embedding to the list\u001b[39;00m\n\u001b[32m     28\u001b[39m     embeddings.append(embedding)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_embedding_for_variable\u001b[39m\u001b[34m(variable_data)\u001b[39m\n\u001b[32m      5\u001b[39m context = context.unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Add a batch dimension (making it a 2D tensor)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Generate embeddings for the time-series data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m embeddings, tokenizer_state = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Extract embeddings for the variable\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/chronos/chronos.py:440\u001b[39m, in \u001b[36mChronosPipeline.embed\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    436\u001b[39m context_tensor = \u001b[38;5;28mself\u001b[39m._prepare_and_validate_context(context=context)\n\u001b[32m    437\u001b[39m token_ids, attention_mask, tokenizer_state = (\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer.context_input_transform(context_tensor)\n\u001b[32m    439\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, tokenizer_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/chronos/chronos.py:308\u001b[39m, in \u001b[36mChronosModel.encode\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03mExtract the encoder embedding for the given token sequences.\u001b[39;00m\n\u001b[32m    289\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m \u001b[33;03m    (batch_size, sequence_length, d_model).\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.model_type == \u001b[33m\"\u001b[39m\u001b[33mseq2seq\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mEncoder embeddings are only supported for encoder-decoder models\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1107\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1108\u001b[39m         layer_module.forward,\n\u001b[32m   1109\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1121\u001b[39m         cache_position,\n\u001b[32m   1122\u001b[39m     )\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:725\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    722\u001b[39m     attention_outputs = attention_outputs + cross_attention_outputs[\u001b[32m2\u001b[39m:]\n\u001b[32m    724\u001b[39m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden_states.dtype == torch.float16:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:339\u001b[39m, in \u001b[36mT5LayerFF.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    338\u001b[39m     forwarded_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     forwarded_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(forwarded_states)\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:284\u001b[39m, in \u001b[36mT5DenseActDense.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.act(hidden_states)\n\u001b[32m    286\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loop through all unique patients (recordID) and get their embeddings\n",
    "all_patient_embeddings = {}\n",
    "\n",
    "for patient_id in patient_data['RecordID'].unique()[:]:\n",
    "    # Get the aggregated embedding for the current patient\n",
    "    patient_embedding = get_patient_embedding(patient_data, patient_id)\n",
    "    \n",
    "    # Store the aggregated embedding for each patient in a dictionary\n",
    "    all_patient_embeddings[patient_id] = patient_embedding\n",
    "    torch.save(all_patient_embeddings, 'train_pat_embeddings.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4aca8-8ada-4c1d-b7ed-3d457d956e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the full object (not just weights) from the .pth file\n",
    "embeddings_tensor = torch.load('train_pat_embeddings.pth', map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "print(len(embeddings_tensor.items()))\n",
    "\n",
    "# Loop through all patient embeddings and check their sizes\n",
    "#for patient_id, embedding in embeddings_tensor.items():\n",
    "    #if isinstance(embedding, torch.Tensor):\n",
    "        #print(f\"Patient ID: {patient_id}, Embedding size: {embedding.size()}\")\n",
    "    #else:\n",
    "        #print(f\"Patient ID: {patient_id}, Embedding is not a tensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675428a-516d-458d-b374-cf5377c9bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
